{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prostate-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "patient-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"breast_drug_cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "labeled-bandwidth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DRUG_ID</th>\n",
       "      <th>COSMIC_ID</th>\n",
       "      <th>CCLE_Name</th>\n",
       "      <th>TSPAN6 (7105)</th>\n",
       "      <th>TNMD (64102)</th>\n",
       "      <th>DPM1 (8813)</th>\n",
       "      <th>SCYL3 (57147)</th>\n",
       "      <th>C1orf112 (55732)</th>\n",
       "      <th>FGR (2268)</th>\n",
       "      <th>CFH (3075)</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1003</td>\n",
       "      <td>909907</td>\n",
       "      <td>ZR7530_BREAST</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>909907</td>\n",
       "      <td>ZR7530_BREAST</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>909907</td>\n",
       "      <td>ZR7530_BREAST</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1006</td>\n",
       "      <td>909907</td>\n",
       "      <td>ZR7530_BREAST</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1007</td>\n",
       "      <td>909907</td>\n",
       "      <td>ZR7530_BREAST</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323</th>\n",
       "      <td>2045</td>\n",
       "      <td>908121</td>\n",
       "      <td>MDAMB361_BREAST</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>2046</td>\n",
       "      <td>908121</td>\n",
       "      <td>MDAMB361_BREAST</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>2048</td>\n",
       "      <td>908121</td>\n",
       "      <td>MDAMB361_BREAST</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>2106</td>\n",
       "      <td>908121</td>\n",
       "      <td>MDAMB361_BREAST</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>2169</td>\n",
       "      <td>908121</td>\n",
       "      <td>MDAMB361_BREAST</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4328 rows × 19484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DRUG_ID  COSMIC_ID        CCLE_Name  TSPAN6 (7105)  TNMD (64102)  \\\n",
       "0        1003     909907    ZR7530_BREAST       3.472488           0.0   \n",
       "1        1004     909907    ZR7530_BREAST       3.472488           0.0   \n",
       "2        1005     909907    ZR7530_BREAST       3.472488           0.0   \n",
       "3        1006     909907    ZR7530_BREAST       3.472488           0.0   \n",
       "4        1007     909907    ZR7530_BREAST       3.472488           0.0   \n",
       "...       ...        ...              ...            ...           ...   \n",
       "4323     2045     908121  MDAMB361_BREAST       1.855990           0.0   \n",
       "4324     2046     908121  MDAMB361_BREAST       1.855990           0.0   \n",
       "4325     2048     908121  MDAMB361_BREAST       1.855990           0.0   \n",
       "4326     2106     908121  MDAMB361_BREAST       1.855990           0.0   \n",
       "4327     2169     908121  MDAMB361_BREAST       1.855990           0.0   \n",
       "\n",
       "      DPM1 (8813)  SCYL3 (57147)  C1orf112 (55732)  FGR (2268)  CFH (3075)  \\\n",
       "0        5.959306       3.878725          3.646163    0.000000    0.042644   \n",
       "1        5.959306       3.878725          3.646163    0.000000    0.042644   \n",
       "2        5.959306       3.878725          3.646163    0.000000    0.042644   \n",
       "3        5.959306       3.878725          3.646163    0.000000    0.042644   \n",
       "4        5.959306       3.878725          3.646163    0.000000    0.042644   \n",
       "...           ...            ...               ...         ...         ...   \n",
       "4323     6.736740       2.885574          3.766595    0.028569    0.879706   \n",
       "4324     6.736740       2.885574          3.766595    0.028569    0.879706   \n",
       "4325     6.736740       2.885574          3.766595    0.028569    0.879706   \n",
       "4326     6.736740       2.885574          3.766595    0.028569    0.879706   \n",
       "4327     6.736740       2.885574          3.766595    0.028569    0.879706   \n",
       "\n",
       "      ...  246  247  248  249  250  251  252  253  254  255  \n",
       "0     ...    0    0    1    0    1    0    0    0    0    0  \n",
       "1     ...    0    1    0    1    0    1    0    0    0    0  \n",
       "2     ...    0    0    0    0    0    0    0    0    0    0  \n",
       "3     ...    0    0    0    0    1    1    0    0    0    0  \n",
       "4     ...    0    0    1    1    1    1    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "4323  ...    0    0    0    0    1    0    0    0    0    0  \n",
       "4324  ...    0    0    0    0    0    1    0    0    0    1  \n",
       "4325  ...    0    1    0    1    1    1    0    0    0    0  \n",
       "4326  ...    0    0    0    1    0    0    0    0    0    1  \n",
       "4327  ...    1    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[4328 rows x 19484 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "optical-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['CCLE_Name', 'DRUG_NAME', 'isosmiles','PubCHEM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lyric-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove drug id column's data\n",
    "df.drop(to_drop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "closing-corporation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DRUG_ID</th>\n",
       "      <th>COSMIC_ID</th>\n",
       "      <th>TSPAN6 (7105)</th>\n",
       "      <th>TNMD (64102)</th>\n",
       "      <th>DPM1 (8813)</th>\n",
       "      <th>SCYL3 (57147)</th>\n",
       "      <th>C1orf112 (55732)</th>\n",
       "      <th>FGR (2268)</th>\n",
       "      <th>CFH (3075)</th>\n",
       "      <th>FUCA2 (2519)</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1003</td>\n",
       "      <td>909907</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>4.711495</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>909907</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>4.711495</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>909907</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>4.711495</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1006</td>\n",
       "      <td>909907</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>4.711495</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1007</td>\n",
       "      <td>909907</td>\n",
       "      <td>3.472488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959306</td>\n",
       "      <td>3.878725</td>\n",
       "      <td>3.646163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>4.711495</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323</th>\n",
       "      <td>2045</td>\n",
       "      <td>908121</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>5.773733</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>2046</td>\n",
       "      <td>908121</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>5.773733</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>2048</td>\n",
       "      <td>908121</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>5.773733</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>2106</td>\n",
       "      <td>908121</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>5.773733</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>2169</td>\n",
       "      <td>908121</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.736740</td>\n",
       "      <td>2.885574</td>\n",
       "      <td>3.766595</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.879706</td>\n",
       "      <td>5.773733</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4328 rows × 19480 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DRUG_ID  COSMIC_ID  TSPAN6 (7105)  TNMD (64102)  DPM1 (8813)  \\\n",
       "0        1003     909907       3.472488           0.0     5.959306   \n",
       "1        1004     909907       3.472488           0.0     5.959306   \n",
       "2        1005     909907       3.472488           0.0     5.959306   \n",
       "3        1006     909907       3.472488           0.0     5.959306   \n",
       "4        1007     909907       3.472488           0.0     5.959306   \n",
       "...       ...        ...            ...           ...          ...   \n",
       "4323     2045     908121       1.855990           0.0     6.736740   \n",
       "4324     2046     908121       1.855990           0.0     6.736740   \n",
       "4325     2048     908121       1.855990           0.0     6.736740   \n",
       "4326     2106     908121       1.855990           0.0     6.736740   \n",
       "4327     2169     908121       1.855990           0.0     6.736740   \n",
       "\n",
       "      SCYL3 (57147)  C1orf112 (55732)  FGR (2268)  CFH (3075)  FUCA2 (2519)  \\\n",
       "0          3.878725          3.646163    0.000000    0.042644      4.711495   \n",
       "1          3.878725          3.646163    0.000000    0.042644      4.711495   \n",
       "2          3.878725          3.646163    0.000000    0.042644      4.711495   \n",
       "3          3.878725          3.646163    0.000000    0.042644      4.711495   \n",
       "4          3.878725          3.646163    0.000000    0.042644      4.711495   \n",
       "...             ...               ...         ...         ...           ...   \n",
       "4323       2.885574          3.766595    0.028569    0.879706      5.773733   \n",
       "4324       2.885574          3.766595    0.028569    0.879706      5.773733   \n",
       "4325       2.885574          3.766595    0.028569    0.879706      5.773733   \n",
       "4326       2.885574          3.766595    0.028569    0.879706      5.773733   \n",
       "4327       2.885574          3.766595    0.028569    0.879706      5.773733   \n",
       "\n",
       "      ...  246  247  248  249  250  251  252  253  254  255  \n",
       "0     ...    0    0    1    0    1    0    0    0    0    0  \n",
       "1     ...    0    1    0    1    0    1    0    0    0    0  \n",
       "2     ...    0    0    0    0    0    0    0    0    0    0  \n",
       "3     ...    0    0    0    0    1    1    0    0    0    0  \n",
       "4     ...    0    0    1    1    1    1    0    0    0    0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "4323  ...    0    0    0    0    1    0    0    0    0    0  \n",
       "4324  ...    0    0    0    0    0    1    0    0    0    1  \n",
       "4325  ...    0    1    0    1    1    1    0    0    0    0  \n",
       "4326  ...    0    0    0    1    0    0    0    0    0    1  \n",
       "4327  ...    1    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[4328 rows x 19480 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fuzzy-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cubic-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target variable (y)\n",
    "X = df.drop(columns=['DRUG_ID', 'COSMIC_ID', 'LN_IC50'])\n",
    "y = df['LN_IC50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "equipped-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "advisory-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bright-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "# Define configurations for neurons and activation functions for each layer\n",
    "neurons_list_layer1 = [64, 128, 256]\n",
    "activation_list_layer1 = ['relu', 'tanh', 'sigmoid']\n",
    "neurons_list_layer2 = [32, 64, 128]\n",
    "activation_list_layer2 = ['relu', 'tanh', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "seven-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store best results\n",
    "best_rmse = float('inf')\n",
    "best_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "introductory-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 1.5782776862003391\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.7489429915300647\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7204427019538056\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 1.4681106837127758\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.729132192792693\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.7243322891030597\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 1.4203650800654344\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.742605676083962\n",
      "Training model with 64 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.6968392742958915\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 2.7098038753107505\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.7010678694478703\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7076638839324834\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 2.697442753214749\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.7121282184605886\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.7126552893176044\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 2.6937810438253837\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.7147336493150664\n",
      "Training model with 64 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.7101327481832946\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 2.718230007097201\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.708781377277203\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7303173868607042\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 2.706249931415434\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.720186759600959\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.720404996151538\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 2.7172888857022\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.7086986478242867\n",
      "Training model with 64 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 64 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.7022878227265443\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 1.508501242791765\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.762908779243825\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.743950133214977\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 1.622119867121696\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.7285977492821485\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.7264526667187434\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 1.5166094554106127\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.718135384009191\n",
      "Training model with 128 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.725149382907356\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 2.689241220797846\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.704932375966393\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7058838125462406\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 2.7216877182069794\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.7073664865729437\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.6964530725544584\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 2.7102396945806313\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.709480010589329\n",
      "Training model with 128 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.694775058226409\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 2.714942041862418\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.6998505566947246\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7067247674430517\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 2.695273770455565\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.6918206790769297\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.709974561719536\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 2.6973783253440615\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.7202489118322197\n",
      "Training model with 128 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 128 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.7268795342655183\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 1.712637978508939\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.7573131127128803\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7659921341374583\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 1.6256380658712042\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.7184359689156485\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.73623750936583\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 1.5546772708251946\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.7285611055640344\n",
      "Training model with 256 neurons and relu activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and relu activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.71372934177653\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 2.7209384757688264\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.7088974875450083\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7048994424500825\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 2.7039300041392025\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.7143975573196015\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.6996902861066405\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 2.7118535704337114\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.707970208957861\n",
      "Training model with 256 neurons and tanh activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and tanh activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.700748058487723\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and relu activation function for layer 2: 2.691249695856301\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and tanh activation function for layer 2: 2.6920497034118607\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 32 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 32 neurons and sigmoid activation function for layer 2: 2.7147012521328255\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and relu activation function for layer 2: 2.7021512674884924\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and tanh activation function for layer 2: 2.6932751963514283\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 64 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 64 neurons and sigmoid activation function for layer 2: 2.692136023762292\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and relu activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and relu activation function for layer 2: 2.7021715673573206\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and tanh activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and tanh activation function for layer 2: 2.7051117289539333\n",
      "Training model with 256 neurons and sigmoid activation function for layer 1...\n",
      "                   and 128 neurons and sigmoid activation function for layer 2...\n",
      "RMSE for 256 neurons and sigmoid activation function for layer 1,\n",
      "            and 128 neurons and sigmoid activation function for layer 2: 2.710574089182925\n",
      "Best configuration found: (64, 'relu', 128, 'relu')\n",
      "Best RMSE: 1.4203650800654344\n"
     ]
    }
   ],
   "source": [
    "# Loop over configurations for layer 1\n",
    "for neurons_layer1 in neurons_list_layer1:\n",
    "    for activation_layer1 in activation_list_layer1:\n",
    "        # Loop over configurations for layer 2\n",
    "        for neurons_layer2 in neurons_list_layer2:\n",
    "            for activation_layer2 in activation_list_layer2:\n",
    "                print(f\"Training model with {neurons_layer1} neurons and {activation_layer1} activation function for layer 1...\")\n",
    "                print(f\"                   and {neurons_layer2} neurons and {activation_layer2} activation function for layer 2...\")\n",
    "                \n",
    "                # Build the deep learning model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neurons_layer1, activation=activation_layer1, input_shape=(X_train.shape[1],)))\n",
    "                model.add(Dense(neurons_layer2, activation=activation_layer2))\n",
    "                model.add(Dense(1))\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "                # Early stopping\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "                # Train the model with early stopping\n",
    "                model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "                # Predict on the test set\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                \n",
    "                # Convert predictions to numpy arrays\n",
    "                y_test_np = np.array(y_test)\n",
    "                y_pred_np = np.squeeze(y_pred)\n",
    "\n",
    "                # Remove NaN values from predictions\n",
    "                nan_indices = np.isnan(y_pred_np)\n",
    "                y_test_np = y_test_np[~nan_indices]\n",
    "                y_pred_np = y_pred_np[~nan_indices]\n",
    "\n",
    "                # Evaluation metrics\n",
    "                rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_np))\n",
    "                \n",
    "                # Print RMSE for current configuration\n",
    "                print(f\"RMSE for {neurons_layer1} neurons and {activation_layer1} activation function for layer 1,\")\n",
    "                print(f\"            and {neurons_layer2} neurons and {activation_layer2} activation function for layer 2: {rmse}\")\n",
    "\n",
    "                # Check if current configuration is the best\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    best_config = (neurons_layer1, activation_layer1, neurons_layer2, activation_layer2)\n",
    "\n",
    "print(\"Best configuration found:\", best_config)\n",
    "print(\"Best RMSE:\", best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "durable-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the deep learning model using the best parameters\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "portable-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "otherwise-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "plastic-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "87/87 [==============================] - 2s 18ms/step - loss: 1.4706 - val_loss: 1.9162\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.4053 - val_loss: 1.9930\n",
      "Epoch 3/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.4488 - val_loss: 1.9762\n",
      "Epoch 4/100\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 1.5021 - val_loss: 2.0640\n",
      "Epoch 5/100\n",
      "87/87 [==============================] - 1s 14ms/step - loss: 1.4077 - val_loss: 1.8767\n",
      "Epoch 6/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.3327 - val_loss: 2.0573\n",
      "Epoch 7/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.3380 - val_loss: 2.0642\n",
      "Epoch 8/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.4284 - val_loss: 2.0299\n",
      "Epoch 9/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.3282 - val_loss: 1.9664\n",
      "Epoch 10/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.2795 - val_loss: 1.9687\n",
      "Epoch 11/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.2780 - val_loss: 2.2195\n",
      "Epoch 12/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.2410 - val_loss: 1.8819\n",
      "Epoch 13/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1718 - val_loss: 1.7479\n",
      "Epoch 14/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1758 - val_loss: 1.9450\n",
      "Epoch 15/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1772 - val_loss: 1.9284\n",
      "Epoch 16/100\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 1.1989 - val_loss: 1.8730\n",
      "Epoch 17/100\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 1.2079 - val_loss: 1.9207\n",
      "Epoch 18/100\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 1.2212 - val_loss: 1.8768\n",
      "Epoch 19/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1073 - val_loss: 2.1277\n",
      "Epoch 20/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1056 - val_loss: 1.8357\n",
      "Epoch 21/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1735 - val_loss: 1.7850\n",
      "Epoch 22/100\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 1.1098 - val_loss: 1.8966\n",
      "Epoch 23/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.0878 - val_loss: 1.7055\n",
      "Epoch 24/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.0598 - val_loss: 1.8442\n",
      "Epoch 25/100\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 1.1676 - val_loss: 1.8053\n",
      "Epoch 26/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.0470 - val_loss: 1.8330\n",
      "Epoch 27/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.0490 - val_loss: 1.7707\n",
      "Epoch 28/100\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 1.1629 - val_loss: 1.8083\n",
      "Epoch 29/100\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 1.0725 - val_loss: 1.7446\n",
      "Epoch 30/100\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 1.1837 - val_loss: 1.9569\n",
      "Epoch 31/100\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 1.1144 - val_loss: 1.7884\n",
      "Epoch 32/100\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 1.1084 - val_loss: 1.8888\n",
      "Epoch 33/100\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 1.0270 - val_loss: 1.8313\n"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ethical-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "other-journalist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 1.330331297938056\n",
      "Mean Absolute Error (MAE): 0.9968494775014761\n",
      "Mean Squared Error (MSE): 1.7697813622735528\n",
      "Pearson Correlation: 0.8828053773345808\n",
      "R-squared (R2): 0.7761309605634303\n"
     ]
    }
   ],
   "source": [
    "# Convert predictions to numpy arrays\n",
    "y_test_np = np.array(y_test)\n",
    "y_pred_np = np.squeeze(y_pred)\n",
    "\n",
    "# Remove NaN values from predictions\n",
    "nan_indices = np.isnan(y_pred_np)\n",
    "y_test_np = y_test_np[~nan_indices]\n",
    "y_pred_np = y_pred_np[~nan_indices]\n",
    "\n",
    "# Evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test_np, y_pred_np))\n",
    "mae = mean_absolute_error(y_test_np, y_pred_np)\n",
    "mse = mean_squared_error(y_test_np, y_pred_np)\n",
    "pearson_corr, _ = pearsonr(y_test_np, y_pred_np)\n",
    "r2 = r2_score(y_test_np, y_pred_np)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Pearson Correlation:\", pearson_corr)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-harmony",
   "metadata": {},
   "source": [
    "**Root Mean Squared Error (RMSE):** <br>\n",
    "RMSE measures the average deviation of the predicted values from the actual values. It is the square root of the average of the squared differences between predicted and actual values. In this case, the RMSE is approximately 1.33, which means that, on average, the predicted LN_IC50 values deviate from the actual values by around 1.33 units.\n",
    "\n",
    "**Mean Absolute Error (MAE):** <br>\n",
    "MAE measures the average absolute deviation of the predicted values from the actual values. It is the average of the absolute differences between predicted and actual values. In this case, the MAE is approximately 0.997, indicating that, on average, the predicted LN_IC50 values deviate from the actual values by around 0.997 units.\n",
    "\n",
    "**Mean Squared Error (MSE):** <br>\n",
    "MSE measures the average of the squared differences between predicted and actual values. In this case, the MSE is approximately 1.77, which is the average squared deviation of the predicted values from the actual values.\n",
    "\n",
    "**Pearson Correlation:** <br>\n",
    "Pearson correlation coefficient measures the linear correlation between predicted and actual values. It ranges from -1 to 1, where 1 indicates a perfect positive linear relationship, 0 indicates no linear relationship, and -1 indicates a perfect negative linear relationship. Here, the Pearson correlation coefficient is approximately 0.883, indicating a strong positive linear correlation between predicted and actual values.\n",
    "\n",
    "**R-squared (R2):** <br>\n",
    "R-squared is a measure of how well the variation in the dependent variable (LN_IC50) is explained by the independent variables (features). It ranges from 0 to 1, where 1 indicates that all the variability in the dependent variable is explained by the independent variables. In this case, R-squared is approximately 0.776, indicating that around 77.6% of the variability in LN_IC50 is explained by the features used in the model.\n",
    "\n",
    "Overall, these metrics indicate that the model performs reasonably well in predicting LN_IC50 values, with relatively low errors (RMSE and MAE), a strong linear correlation (Pearson correlation), and a moderate level of explained variability (R-squared)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
